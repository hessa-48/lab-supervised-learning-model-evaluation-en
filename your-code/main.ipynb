{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Model Evaluation Lab\n",
    "\n",
    "Complete the exercises below to solidify your knowledge and understanding of supervised learning model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "data = pd.read_csv('housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CRIM - per capita crime rate by town\n",
    "ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "INDUS - proportion of non-retail business acres per town.\n",
    "CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "NOX - nitric oxides concentration (parts per 10 million)\n",
    "RM - average number of rooms per dwelling\n",
    "AGE - proportion of owner-occupied units built prior to 1940\n",
    "DIS - weighted distances to five Boston employment centres\n",
    "RAD - index of accessibility to radial highways\n",
    "TAX - full-value property-tax rate per $10,000\n",
    "PTRATIO - pupil-teacher ratio by town\n",
    "B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "LSTAT - % lower status of the population\n",
    "MEDV - Median value of owner-occupied homes in $1000's\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split this data set into training (80%) and testing (20%) sets.\n",
    "\n",
    "The `MEDV` field represents the median value of owner-occupied homes (in $1000's) and is the target variable that we will want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the column names\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "\n",
    "# Load the dataset with specified column names and space delimiter\n",
    "data = pd.read_csv('housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "\n",
    "# Separate the features (X) and target (y)\n",
    "X = data.drop('MEDV', axis=1)  # Features (all columns except 'MEDV')\n",
    "y = data['MEDV']  # Target (the 'MEDV' column)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a `LinearRegression` model on this data set and generate predictions on both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the column names\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "\n",
    "# Load the dataset with specified column names and space delimiter\n",
    "data = pd.read_csv('housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "\n",
    "# Separate the features (X) and target (y)\n",
    "X = data.drop('MEDV', axis=1)  # Features (all columns except 'MEDV')\n",
    "y = data['MEDV']  # Target (the 'MEDV' column)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on both the training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model: Mean Squared Error and R-squared for both training and testing sets\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Training Mean Squared Error: {train_mse}\")\n",
    "print(f\"Testing Mean Squared Error: {test_mse}\")\n",
    "print(f\"Training R-squared: {train_r2}\")\n",
    "print(f\"Testing R-squared: {test_r2}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate and print R-squared for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate R-squared for both training and testing sets\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print R-squared for both sets\n",
    "print(f\"Training R-squared: {train_r2}\")\n",
    "print(f\"Testing R-squared: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate and print mean squared error for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate Mean Squared Error for both training and testing sets\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# Print the Mean Squared Error for both sets\n",
    "print(f\"Training Mean Squared Error: {train_mse}\")\n",
    "print(f\"Testing Mean Squared Error: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate and print mean absolute error for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Calculate Mean Absolute Error for both training and testing sets\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "# Print the Mean Absolute Error for both sets\n",
    "print(f\"Training Mean Absolute Error: {train_mae}\")\n",
    "print(f\"Testing Mean Absolute Error: {test_mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data['data'],columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.DataFrame(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split this data set into training (80%) and testing (20%) sets.\n",
    "\n",
    "The `class` field represents the type of flower and is the target variable that we will want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the target variable as a new column 'class' to the DataFrame\n",
    "df['class'] = data.target\n",
    "\n",
    "# Check the DataFrame again to confirm the 'class' column is added\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df.drop('class', axis=1)  # Features (drop the 'class' column from df)\n",
    "y = df['class']  # Target (class column)\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the size of the training and testing sets to confirm the split\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train a `LogisticRegression` model on this data set and generate predictions on both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for the training set\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "\n",
    "# Generate predictions for the testing set\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "\n",
    "# Print predictions (optional)\n",
    "print(\"Training set predictions:\", y_train_pred[:5])  # First 5 predictions for training set\n",
    "print(\"Testing set predictions:\", y_test_pred[:5])    # First 5 predictions for testing set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calculate and print the accuracy score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate the accuracy for the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate the accuracy for the testing set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the accuracy scores\n",
    "print(f\"Training set accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing set accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calculate and print the balanced accuracy score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Calculate the balanced accuracy for the training set\n",
    "train_balanced_accuracy = balanced_accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate the balanced accuracy for the testing set\n",
    "test_balanced_accuracy = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the balanced accuracy scores\n",
    "print(f\"Training set balanced accuracy: {train_balanced_accuracy:.4f}\")\n",
    "print(f\"Testing set balanced accuracy: {test_balanced_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Calculate and print the precision score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Calculate the precision for the training set (using 'macro' average for multi-class)\n",
    "train_precision = precision_score(y_train, y_train_pred, average='macro')\n",
    "\n",
    "# Calculate the precision for the testing set (using 'macro' average for multi-class)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "# Print the precision scores\n",
    "print(f\"Training set precision: {train_precision:.4f}\")\n",
    "print(f\"Testing set precision: {test_precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Calculate and print the recall score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Calculate the recall for the training set (using 'macro' average for multi-class)\n",
    "train_recall = recall_score(y_train, y_train_pred, average='macro')\n",
    "\n",
    "# Calculate the recall for the testing set (using 'macro' average for multi-class)\n",
    "test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "# Print the recall scores\n",
    "print(f\"Training set recall: {train_recall:.4f}\")\n",
    "print(f\"Testing set recall: {test_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Calculate and print the F1 score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate the F1 score for the training set (using 'macro' average for multi-class)\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "\n",
    "# Calculate the F1 score for the testing set (using 'macro' average for multi-class)\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "# Print the F1 scores\n",
    "print(f\"Training set F1 score: {train_f1:.4f}\")\n",
    "print(f\"Testing set F1 score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Generate confusion matrices for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate confusion matrix for the training set\n",
    "train_cm = confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "# Generate confusion matrix for the testing set\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot the confusion matrix for the training set\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(train_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=data['target_names'], yticklabels=data['target_names'])\n",
    "plt.title(\"Confusion Matrix for Training Set\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the confusion matrix for the testing set\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(test_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=data['target_names'], yticklabels=data['target_names'])\n",
    "plt.title(\"Confusion Matrix for Testing Set\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: For each of the data sets in this lab, try training with some of the other models you have learned about, recalculate the evaluation metrics, and compare to determine which models perform best on each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have fun here !\n",
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=200),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Generate predictions on training and testing sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics for training and testing sets\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_precision = precision_score(y_train, y_train_pred, average='macro')\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "    \n",
    "    train_recall = recall_score(y_train, y_train_pred, average='macro')\n",
    "    test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "    \n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "    \n",
    "    # Store the results\n",
    "    results[model_name] = {\n",
    "        'Train Accuracy': train_accuracy,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Train Precision': train_precision,\n",
    "        'Test Precision': test_precision,\n",
    "        'Train Recall': train_recall,\n",
    "        'Test Recall': test_recall,\n",
    "        'Train F1': train_f1,\n",
    "        'Test F1': test_f1,\n",
    "        'Train Confusion Matrix': confusion_matrix(y_train, y_train_pred),\n",
    "        'Test Confusion Matrix': confusion_matrix(y_test, y_test_pred)\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Train Accuracy: {metrics['Train Accuracy']:.4f}\")\n",
    "    print(f\"Test Accuracy: {metrics['Test Accuracy']:.4f}\")\n",
    "    print(f\"Train Precision: {metrics['Train Precision']:.4f}\")\n",
    "    print(f\"Test Precision: {metrics['Test Precision']:.4f}\")\n",
    "    print(f\"Train Recall: {metrics['Train Recall']:.4f}\")\n",
    "    print(f\"Test Recall: {metrics['Test Recall']:.4f}\")\n",
    "    print(f\"Train F1: {metrics['Train F1']:.4f}\")\n",
    "    print(f\"Test F1: {metrics['Test F1']:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrices for training and testing sets\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(metrics['Train Confusion Matrix'], annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=data['target_names'], yticklabels=data['target_names'])\n",
    "    plt.title(f\"Train Confusion Matrix - {model_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(metrics['Test Confusion Matrix'], annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=data['target_names'], yticklabels=data['target_names'])\n",
    "    plt.title(f\"Test Confusion Matrix - {model_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
